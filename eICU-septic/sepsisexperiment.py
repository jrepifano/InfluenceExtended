# -*- coding: utf-8 -*-
"""sepsisexperiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nfmewPb3xya2ggWM9nBPMVgow_nzFCAN

bold text
"""

from google.colab import drive
drive.mount('/content/drive/')

from __future__ import print_function

import pandas as pd
pd.__version__

! apt-get install default-jre
!java -version

#I filtered the patients to only include patients with infections/sepsis 
df= pd.read_csv('/content/drive/My Drive/122.csv')

df.nunique()

df.info()

df.head()

df=df.drop(axis=1,columns='saps3day1')

df=df.drop_duplicates(subset='patientunitstayid', keep='first')

df.info()

pip install h2o

pip install datacleaner

from datacleaner import autoclean
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from scipy import stats

my_clean_data = autoclean(df)

df=pd.DataFrame(my_clean_data)

df.describe()

pip install autoviz

from autoviz.AutoViz_Class import AutoViz_Class

AV = AutoViz_Class()

import numpy as np
import matplotlib.pyplot as plt

import h2o
h2o.init()

df['day1fio2']=df['day1fio2']/100

df.head()

df['day1p/f']=df['day1pao2']/df['day1fio2']

df=df.drop(axis=1,columns='patientunitstayid')

df=df.drop(axis=1,columns='admitdiagnosis')

from h2o.automl import H2OAutoML

y = df.iloc[:, 19].values

df=pd.DataFrame(df)

df.info()

df=h2o.H2OFrame(df)

X =  df[[	'day1motor','day1p/f','ALBUMIN_min','age',		
         'CREATININE_max',	'CHLORIDE_max',	'CHLORIDE_min',	'GLUCOSE_min'	,'HEMATOCRIT_min',	'LACTATE_max',	'PLATELET_min','PTT_max',	
         'INR_max',	'PT_max', 'BILIRUBIN_max','WBC_min','WBC_max', 'SODIUM_min', 'SODIUM_max','aids','diabetes','hepaticfailure','lymphoma','metastaticcancer','leukemia','immunosuppression']]

from h2o.automl import H2OAutoML, get_leaderboard

X=df[[ 'ALBUMIN_min', 'CREATININE_max',
       'INR_max', 'GLUCOSE_min', 'BILIRUBIN_max', 'LACTATE_max',
       'PLATELET_min', 'age', 'day1motor', 'day1p/f']]

df['actualicumortality'] = df['actualicumortality'].asfactor()

y=df['actualicumortality']
x = list(X.columns)

split = df.split_frame(ratios = [0.8], seed = 1234)
train = split[0] # using 80% for training
test = split[1] #rest 20% for testing
predictors = list(X.columns)

aml = H2OAutoML(max_models=10,stopping_metric='AUC' ,sort_metric='AUC',balance_classes = True,seed=1)
aml.train(x=predictors, y='actualicumortality',training_frame=train)

aml=H2OAutoML(sort_metric='AUC',balance_classes=True,max_after_balance_size=3.0)

aml.train(x=predictors, y='actualicumortality',training_frame=train)

lb = aml.leaderboard

lb

model_ids = list(aml.leaderboard['model_id'].as_data_frame().iloc[:,0])
m = h2o.get_model([mid for mid in model_ids if "XGB" in mid][0])
m

sep = ','
dft = AV.AutoViz(filename="",sep=sep, depVar='actualicumortality', dfte=df, header=0, verbose=3, 
                 lowess=False, chart_format='svg', max_rows_analyzed=260000, max_cols_analyzed=30)



import sklearn.model_selection
import sklearn.datasets
import sklearn.metrics

X_train, X_test, y_train, y_test = \
        sklearn.model_selection.train_test_split(X, y, random_state=1)

from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier

import numpy as np
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier

train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)

train_X=pd.DataFrame(train_X)
val_X= pd.DataFrame(val_X)

my_model = RandomForestClassifier(n_estimators=100,
                                  random_state=0).fit(train_X, train_y)

pip install eli5

import eli5

from eli5.sklearn import PermutationImportance

perm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)
eli5.show_weights(perm, feature_names = val_X.columns.tolist())

pip install shap

row_to_show = 99
data_for_prediction = val_X.iloc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired
data_for_prediction_array = data_for_prediction.values.reshape(1, -1)


my_model.predict_proba(data_for_prediction_array)

import shap  # package used to calculate Shap values

# Create object that can calculate shap values
explainer = shap.TreeExplainer(my_model)

# Calculate Shap values
shap_values = explainer.shap_values(data_for_prediction)

shap.initjs()
shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)

shap_values = explainer.shap_values(val_X)

# Make plot. Index of [1] is explained in text below.
shap.summary_plot(shap_values[1], val_X)

import shap
shap_values = shap.TreeExplainer(my_model).shap_values(val_X)
shap.summary_plot(shap_values, val_X, plot_type="bar")

pip install pdpbox

import pdpbox

from pdpbox import pdp, get_dataset, info_plots
# Create the data that we will plot
pdp_lactate = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=feature_names, feature='LACTATE_max')

# plot it
pdp.pdp_plot(pdp_lactate, 'LACTATE_max')
plt.show()

from pdpbox import pdp, get_dataset, info_plots
# Create the data that we will plot
pdp_age = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=feature_names, feature='age')

# plot it
pdp.pdp_plot(pdp_age, 'age')
plt.show()

pdp_platelet = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=feature_names, feature='PLATELET_min')

# plot it
pdp.pdp_plot(pdp_age, 'PLATELET_min')
plt.show()

features_to_plot = ['age', 'LACTATE_max']
inter1  =  pdp.pdp_interact(model=tree_model, dataset=val_X, model_features=feature_names, features=features_to_plot)

pdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour', plot_pdp=True)
plt.show()

X = pd.DataFrame(X)
y = pd.DataFrame(y)

X_train, X_test, y_train, y_test = \
        sklearn.model_selection.train_test_split(X, y, random_state=1)

corr_matrix = X_train.corr(method = "spearman").abs()
# Draw the heatmap
sns.set(font_scale = 1.0)
f, ax = plt.subplots(figsize=(11, 9))
sns.heatmap(corr_matrix, cmap= "YlGnBu", square=True, ax = ax)
f.tight_layout()
plt.savefig("correlation_matrix.png", dpi = 1080)

# Select upper triangle of matrix
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))

# Find index of feature columns with correlation greater than 0.95
to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]

# Drop features
X_train = X_train.drop(to_drop, axis = 1)
X_test = X_test.drop(to_drop, axis = 1)

X_train

pip install catboost

from catboost import CatBoostClassifier, Pool

#comparing basic catboost with the pipelines
model = CatBoostClassifier(iterations=2,
                           depth=2,
                           learning_rate=1,
                           loss_function='Logloss',
                           verbose=True)

model.fit(X_train, y_train)
# make the prediction using the resulting model
preds_class = model.predict(X_test)
preds_proba = model.predict_proba(X_test)
print("class = ", preds_class)
print("proba = ", preds_proba)

#score for all 13 parameters
print(model.score(X_test, y_test))

X_train= pd.DataFrame(X_train)
y_train=pd.DataFrame(y_train)

from sklearn.dummy import DummyClassifier

dummy_clf = DummyClassifier(strategy="most_frequent")
dummy_clf.fit(X_train, y_train)
DummyClassifier(strategy='most_frequent')
dummy_clf.predict(X_test)
dummy_clf.score(X_test, y_test)

X_train, X_test, y_train, y_test = \
        sklearn.model_selection.train_test_split(X, y, random_state=1)

from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)

from sklearn.dummy import DummyClassifier

pip install sklearn

pip install tpot

from tpot import TPOTClassifier

tpot = TPOTClassifier(generations=1, population_size=50, max_time_mins=240, verbosity=3)

tpot.fit(X_train, y_train)

print(tpot.score(X_test, y_test))

tpot.export('tpot_exported_pipeline.py')

print(tpot.fitted_pipeline_)

model = CatBoostClassifier(iterations=2,
                           depth=2,
                           learning_rate=1,
                           loss_function='Logloss',
                           verbose=True)

model.fit(X_train, y_train)
# make the prediction using the resulting model
preds_class = model.predict(X_test)
preds_proba = model.predict_proba(X_test)
print("class = ", preds_class)
print("proba = ", preds_proba)

#score for the chose 3 features/lactate/platelets/age, appears to be the same.  
print(model.score(X_test, y_test))

pip install yellowbrick

pip install shap

from yellowbrick.datasets import load_credit
from yellowbrick.features import Rank1D

visualizer = Rank1D(algorithm='shapiro')

visualizer.fit(X_train, y_train)           # Fit the data to the visualizer
visualizer.transform(X_train)        # Transform the data
visualizer.show()

from sklearn.metrics import accuracy_score, log_loss
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC, LinearSVC, NuSVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())])
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])

rf=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse',init=None,
                           learning_rate=0.1, loss='deviance', max_depth=6,
                           max_features=0.6000000000000001, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=18, min_samples_split=16,
                           min_weight_fraction_leaf=0.0, n_estimators=100,
                           n_iter_no_change=None, presort='deprecated',
                           random_state=None, subsample=0.55, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)

rf.fit(X_train, y_train)

"""rf.fit(X_train, y_train)"""

#KNN scores
print(rf.score(X_test, y_test))

from sklearn.model_selection import cross_val_score
import numpy as np
#create a new KNN model
knn_cv = rf
#train model with cv of 5 
cv_scores = cross_val_score(rf, X_train, y_train, cv=10)
#print each cv score (accuracy) and average them
print(cv_scores)

print(rf.score(X_train,y_train))

y_hat=rf.predict(X_test)

from sklearn.metrics import roc_auc_score

gb.fit(X_train, y_train)

print(gb.score(X_test, y_test))



roc_auc_score(y_test, y_hat)

from sklearn.metrics import classification_report
classification_report(y_test, y_hat)

pip install yellowbrick